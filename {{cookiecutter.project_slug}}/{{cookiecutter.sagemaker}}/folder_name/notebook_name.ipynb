# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.11.1
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# # Daily Weather Forecast
#
# This notebook gets the weather forecast for the city requested and shows the forecast and the temperature graph.

# ## The input
#
# This cell will be replaced by the place parameter specified when you run the notebook using notebook execution:

# + tags=["parameters"]
place="Minneapolis, MN"
# -

# ## Convert the location to longitude and latitude
#
# In this section, we use the [geopy](https://geopy.readthedocs.io/en/stable/) library to get the latitude and longitude of the city from Open Street Map. 

from geopy.geocoders import Nominatim
import boto3

geolocator = Nominatim(user_agent="sagemaker test app")
location = geolocator.geocode(place)

if location is None:
    raise ValueError("Unknown place name: {}".format(place))

# +
latitude = location.latitude
longitude = location.longitude


print("{} is at lat/long {}, {}".format(place, latitude, longitude))
# -

# ## Use the NWS API to retrieve the forecast
#
# The National Weather Service has an [API](https://www.weather.gov/documentation/services-web-api) to retrieving forecasts for any lat/long combo in the United States. 
#
# Getting the forecast from an arbitrary point is a two-stage process. First, you call the `points` endpoint to find the forecasts for the specified point. The result gives you URLs for hourly and twice-daily forecasts that cover that point. You retrieve those to get the actual forecast.
#
# All the results are in JSON, so they're easy to consume.

info_url = "https://api.weather.gov/points/{0:.4f},{1:.4f}".format(latitude,longitude)
info_url

# We use the `requests` library to retrieve the JSON and then save it to a pandas dataframe.

import requests
import json
import pandas as pd
import numpy as np
import s3fs

s3 = s3fs.S3FileSystem(s3_additional_kwargs={'ServerSideEncryption': 'AES256'})
response = requests.get(info_url)
# If the response was successful, no Exception will be raised
response.raise_for_status()

info = json.loads(response.text)
forecastHourly_url = info["properties"]["forecastHourly"]
forecast_url = info["properties"]["forecast"]


# +
def get_forecast(url):
    response = requests.get(url)
    response.raise_for_status()

    forecast_data = json.loads(response.text)
    forecast = pd.DataFrame(forecast_data["properties"]["periods"])
    forecast["startTime"] = pd.to_datetime(forecast["startTime"])
    forecast["endTime"] = pd.to_datetime(forecast["endTime"])
    return forecast

hourly_forecast = get_forecast(forecastHourly_url)
daily_forecast = get_forecast(forecast_url)
# -

# ## Forecast for the next 24 hours

today = hourly_forecast[:24][["startTime","temperature","windSpeed", "windDirection", "shortForecast"]]
today = today.rename(columns={"temperature": "Temperature", "shortForecast": "Forecast"})
today.insert(0, "Date", today["startTime"].dt.strftime("%b %d"))
today.insert(1, "Time", today["startTime"].dt.strftime("%H:%M"))
today.insert(4, "Wind", today[["windSpeed", "windDirection"]].agg(' '.join, axis=1 ))
today = today.drop(["startTime", "windSpeed", "windDirection"], axis="columns")
today.set_index(['Date', 'Time'])

# ## Forecast for the next week

week = daily_forecast[["name","temperature","windSpeed", "windDirection", "shortForecast"]]
week = week.rename(columns={"name": "When", "temperature": "Temperature", "shortForecast": "Forecast"})
week.insert(4, "Wind", week[["windSpeed", "windDirection"]].agg(' '.join, axis=1 ))
week = week.drop(["windSpeed", "windDirection"], axis="columns")
week = week.style.hide_index()
week

# ## A chart of temperatures for the next 7 days
#
# We use the [Altair](https://altair-viz.github.io/index.html) charting library to render the hourly predicted temperatures for the next week.

import altair as alt

nights_df=hourly_forecast.assign(new=hourly_forecast.isDaytime.diff().ne(0).cumsum()).groupby(["isDaytime", "new"]).agg({"startTime": min, "endTime": max}).reset_index()
nights_df=nights_df[nights_df["isDaytime"]==False]
nights=alt.Chart(nights_df).mark_rect(color="#9E9CC9").encode(x="startTime:T", x2="endTime:T")

# +
scaleMin = np.min(hourly_forecast["temperature"])
scaleMin = int(scaleMin * 0.9)
scaleMin = scaleMin - (scaleMin % 5)

scaleMax = np.max(hourly_forecast["temperature"])
scaleMax = int(scaleMax * 1.1)
scaleMax = scaleMax + (5 - (scaleMax % 5))
domain = (scaleMin, scaleMax)
# -

temps = (alt.Chart(hourly_forecast, title="7-day Temperature Forecast for {}".format(place)).
         mark_line().
         encode(x=alt.X("startTime", title="Date"),
                y=alt.Y("temperature", title="Temperature (Â°F)",scale=alt.Scale(domain=domain))).
         properties(width=700, height=300))

nights + temps

# ## Save the results with scrapbook
#
# We use [scrapbook](https://github.com/nteract/scrapbook) to save particular results of this run so that they can be retrieved later by reading the notebook.
#
# This will let us take the data from multiple runs of the notebook and compare it.

import scrapbook as sb
import boto3
sb.glue("place", place)
sb.glue("hourly_forecast", hourly_forecast.to_json())
sb.glue("daily_forecast", daily_forecast.to_json())
s3 = boto3.client('s3')
print("done")


